{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš² Bergen Bike Availability Prediction System\n",
    "\n",
    "## AI for Social Good: Sustainable Urban Mobility\n",
    "\n",
    "**Team Members:** Ang Kar Kin, Nicolas Lasch, Chan Kok Siang\n",
    "\n",
    "**Course:** 61.501 Production Ready ML (Y2025)\n",
    "\n",
    "---\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "This project predicts bike availability at bike-sharing stations in Bergen, Norway, enabling:\n",
    "- **Operators** to proactively manage bike distribution\n",
    "- **Users** to find the best station based on location and predicted availability\n",
    "\n",
    "We use interpretable ML models (Random Forest & XGBoost) to forecast hourly trip counts per station.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "def parse_timestamps(df):\n",
    "    df = df.copy()\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_temporal_features(df):\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['start_time'].dt.hour\n",
    "    df['day'] = df['start_time'].dt.day\n",
    "    df['month'] = df['start_time'].dt.month\n",
    "    df['dayofweek'] = df['start_time'].dt.dayofweek\n",
    "    df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_missing_values(df):\n",
    "    df = df.copy()\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df, column, multiplier=3.0):\n",
    "    df = df.copy()\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - multiplier * iqr\n",
    "    upper = q3 + multiplier * iqr\n",
    "    mask = (df[column] >= lower) & (df[column] <= upper)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def preprocess_pipeline(filepath):\n",
    "    df = load_dataset(filepath)\n",
    "    df = parse_timestamps(df)\n",
    "    df = extract_temporal_features(df)\n",
    "    df = clean_missing_values(df)\n",
    "    df = remove_outliers(df, 'duration')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_station_hourly(df):\n",
    "    hourly = df.groupby([\n",
    "        'start_station_id',\n",
    "        'start_station_name',\n",
    "        df['start_time'].dt.date.rename('date'),\n",
    "        'hour'\n",
    "    ]).agg({\n",
    "        'duration': 'count',\n",
    "        'temperature': 'mean',\n",
    "        'max_temperature': 'mean',\n",
    "        'min_temperature': 'mean',\n",
    "        'wind_speed': 'mean',\n",
    "        'precipitation': 'mean',\n",
    "        'humidity': 'mean',\n",
    "        'sunshine': 'mean',\n",
    "        'season': 'first',\n",
    "        'is_holiday': 'first',\n",
    "        'is_weekend': 'first',\n",
    "        'start_station_latitude': 'first',\n",
    "        'start_station_longitude': 'first',\n",
    "        'is_rush_hour': 'first',\n",
    "        'dayofweek': 'first',\n",
    "        'month': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    hourly = hourly.rename(columns={'duration': 'trip_count'})\n",
    "    return hourly\n",
    "\n",
    "\n",
    "def create_lag_features(df, lags=[1, 2, 3, 24]):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['start_station_id', 'date', 'hour'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'trip_count_lag_{lag}'] = df.groupby('start_station_id')['trip_count'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rolling_features(df, windows=[3, 6, 24]):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['start_station_id', 'date', 'hour'])\n",
    "    \n",
    "    for window in windows:\n",
    "        df[f'trip_count_rolling_{window}h'] = df.groupby('start_station_id')['trip_count'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_weather_interactions(df):\n",
    "    df = df.copy()\n",
    "    df['temp_humidity'] = df['temperature'] * df['humidity']\n",
    "    df['temp_wind'] = df['temperature'] * df['wind_speed']\n",
    "    df['rain_wind'] = df['precipitation'] * df['wind_speed']\n",
    "    df['is_rainy'] = (df['precipitation'] > 0).astype(int)\n",
    "    df['is_hot'] = (df['temperature'] > 20).astype(int)\n",
    "    df['is_cold'] = (df['temperature'] < 5).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_cyclic_features(df):\n",
    "    df = df.copy()\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_station_popularity(df):\n",
    "    df = df.copy()\n",
    "    station_stats = df.groupby('start_station_id')['trip_count'].agg(['mean', 'std']).reset_index()\n",
    "    station_stats.columns = ['start_station_id', 'station_avg_trips', 'station_std_trips']\n",
    "    df = df.merge(station_stats, on='start_station_id', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineering_pipeline(df):\n",
    "    df = aggregate_station_hourly(df)\n",
    "    df = create_lag_features(df)\n",
    "    df = create_rolling_features(df)\n",
    "    df = create_weather_interactions(df)\n",
    "    df = encode_cyclic_features(df)\n",
    "    df = calculate_station_popularity(df)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_train_test_split(df, test_ratio=0.2):\n",
    "    df = df.sort_values(['date', 'hour'])\n",
    "    split_idx = int(len(df) * (1 - test_ratio))\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def get_feature_columns():\n",
    "    return [\n",
    "        'temperature', 'max_temperature', 'min_temperature',\n",
    "        'wind_speed', 'precipitation', 'humidity', 'sunshine',\n",
    "        'season', 'is_holiday', 'is_weekend', 'is_rush_hour',\n",
    "        'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'trip_count_lag_1', 'trip_count_lag_2', 'trip_count_lag_3', 'trip_count_lag_24',\n",
    "        'trip_count_rolling_3h', 'trip_count_rolling_6h', 'trip_count_rolling_24h',\n",
    "        'temp_humidity', 'temp_wind', 'rain_wind',\n",
    "        'is_rainy', 'is_hot', 'is_cold',\n",
    "        'station_avg_trips', 'station_std_trips',\n",
    "        'start_station_latitude', 'start_station_longitude'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_xgboost(X_train, y_train, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def hyperparameter_tuning_rf(X_train, y_train, X_val, y_val):\n",
    "    param_grid = [\n",
    "        {'n_estimators': 50, 'max_depth': 10},\n",
    "        {'n_estimators': 100, 'max_depth': 15},\n",
    "        {'n_estimators': 150, 'max_depth': 20},\n",
    "        {'n_estimators': 200, 'max_depth': 25}\n",
    "    ]\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    \n",
    "    for params in param_grid:\n",
    "        model = train_random_forest(X_train, y_train, {**params, 'random_state': 42, 'n_jobs': -1})\n",
    "        metrics = evaluate_model(model, X_val, y_val)\n",
    "        if metrics['RMSE'] < best_score:\n",
    "            best_score = metrics['RMSE']\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def hyperparameter_tuning_xgb(X_train, y_train, X_val, y_val):\n",
    "    param_grid = [\n",
    "        {'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.1},\n",
    "        {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.1},\n",
    "        {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.05},\n",
    "        {'n_estimators': 150, 'max_depth': 12, 'learning_rate': 0.05}\n",
    "    ]\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    \n",
    "    for params in param_grid:\n",
    "        model = train_xgboost(X_train, y_train, {**params, 'random_state': 42, 'n_jobs': -1})\n",
    "        metrics = evaluate_model(model, X_val, y_val)\n",
    "        if metrics['RMSE'] < best_score:\n",
    "            best_score = metrics['RMSE']\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "def compare_models(models, X_test, y_test):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        metrics['model'] = name\n",
    "        results.append(metrics)\n",
    "    return pd.DataFrame(results).set_index('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Station Recommendation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def get_nearby_stations(user_lat, user_lon, stations_df, radius_km=2.0):\n",
    "    stations = stations_df.copy()\n",
    "    stations['distance_km'] = stations.apply(\n",
    "        lambda row: calculate_distance(user_lat, user_lon, row['start_station_latitude'], row['start_station_longitude']),\n",
    "        axis=1\n",
    "    )\n",
    "    nearby = stations[stations['distance_km'] <= radius_km].sort_values('distance_km')\n",
    "    return nearby\n",
    "\n",
    "\n",
    "def calculate_recommendation_score(distance, predicted_trips, max_distance=2.0):\n",
    "    distance_score = 1 - (distance / max_distance)\n",
    "    availability_score = min(predicted_trips / 10, 1.0)\n",
    "    combined_score = 0.4 * distance_score + 0.6 * availability_score\n",
    "    return combined_score\n",
    "\n",
    "\n",
    "def recommend_best_station(user_lat, user_lon, target_hour, stations_info, features_df, model, feature_cols, top_k=3):\n",
    "    unique_stations = stations_info[['start_station_id', 'start_station_name', \n",
    "                                      'start_station_latitude', 'start_station_longitude']].drop_duplicates()\n",
    "    nearby = get_nearby_stations(user_lat, user_lon, unique_stations, radius_km=2.0)\n",
    "    \n",
    "    if nearby.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    recommendations = []\n",
    "    for _, station in nearby.iterrows():\n",
    "        station_features = features_df[\n",
    "            (features_df['start_station_id'] == station['start_station_id']) &\n",
    "            (features_df['hour'] == target_hour)\n",
    "        ]\n",
    "        \n",
    "        if station_features.empty:\n",
    "            continue\n",
    "        \n",
    "        latest_features = station_features.iloc[-1:][feature_cols]\n",
    "        predicted_trips = model.predict(latest_features)[0]\n",
    "        score = calculate_recommendation_score(station['distance_km'], predicted_trips)\n",
    "        \n",
    "        recommendations.append({\n",
    "            'station_id': station['start_station_id'],\n",
    "            'station_name': station['start_station_name'],\n",
    "            'latitude': station['start_station_latitude'],\n",
    "            'longitude': station['start_station_longitude'],\n",
    "            'distance_km': station['distance_km'],\n",
    "            'predicted_trips': predicted_trips,\n",
    "            'recommendation_score': score\n",
    "        })\n",
    "    \n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    rec_df = rec_df.sort_values('recommendation_score', ascending=False).head(top_k)\n",
    "    return rec_df\n",
    "\n",
    "\n",
    "def format_recommendation_output(recommendations):\n",
    "    if recommendations.empty:\n",
    "        return \"No stations found nearby. Try increasing the search radius.\"\n",
    "    \n",
    "    output_lines = [\"Top Station Recommendations:\\n\"]\n",
    "    for idx, row in recommendations.iterrows():\n",
    "        output_lines.append(f\"Station: {row['station_name']}\")\n",
    "        output_lines.append(f\"  Distance: {row['distance_km']:.2f} km\")\n",
    "        output_lines.append(f\"  Predicted Activity: {row['predicted_trips']:.1f} trips/hour\")\n",
    "        output_lines.append(f\"  Score: {row['recommendation_score']:.3f}\")\n",
    "        output_lines.append(\"\")\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_style():\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "def plot_feature_importance(importance_df, top_n=15):\n",
    "    set_plot_style()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    bars = ax.barh(range(len(top_features)), top_features['importance'].values)\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'].values)\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title(f'Top {top_n} Most Important Features')\n",
    "    ax.invert_yaxis()\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, f'{width:.4f}', ha='left', va='center')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred):\n",
    "    set_plot_style()\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5, s=10)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    ax.set_xlabel('Actual Trip Count')\n",
    "    ax.set_ylabel('Predicted Trip Count')\n",
    "    ax.set_title('Actual vs Predicted Trip Counts')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_residuals(y_true, y_pred):\n",
    "    set_plot_style()\n",
    "    residuals = y_true - y_pred\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    axes[0].scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[0].set_xlabel('Predicted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title('Residuals vs Predicted Values')\n",
    "    axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Residual Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of Residuals')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_hourly_patterns(df):\n",
    "    set_plot_style()\n",
    "    hourly_avg = df.groupby('hour')['trip_count'].mean()\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "    ax.fill_between(hourly_avg.index, hourly_avg.values, alpha=0.3)\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average Trip Count')\n",
    "    ax.set_title('Average Bike Trips by Hour of Day')\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_weather_correlation(df):\n",
    "    set_plot_style()\n",
    "    weather_cols = ['temperature', 'humidity', 'wind_speed', 'precipitation', 'sunshine', 'trip_count']\n",
    "    corr_matrix = df[weather_cols].corr()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, \n",
    "                linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax, fmt='.2f')\n",
    "    ax.set_title('Correlation Matrix: Weather vs Trip Count')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_model_comparison(comparison_df):\n",
    "    set_plot_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "    colors = sns.color_palette(\"husl\", len(comparison_df))\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        bars = axes[idx].bar(comparison_df.index, comparison_df[metric], color=colors)\n",
    "        axes[idx].set_title(f'Model Comparison: {metric}')\n",
    "        axes[idx].set_ylabel(metric)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{height:.4f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_station_map(stations_df, recommendations=None, user_location=None):\n",
    "    set_plot_style()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.scatter(stations_df['start_station_longitude'], stations_df['start_station_latitude'], \n",
    "               c='gray', s=20, alpha=0.5, label='All Stations')\n",
    "    if recommendations is not None and not recommendations.empty:\n",
    "        ax.scatter(recommendations['longitude'], recommendations['latitude'],\n",
    "                  c='green', s=100, marker='*', label='Recommended Stations', zorder=5)\n",
    "        for _, rec in recommendations.iterrows():\n",
    "            ax.annotate(f\"{rec['station_name'][:20]}...\", \n",
    "                       (rec['longitude'], rec['latitude']),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    if user_location:\n",
    "        ax.scatter(user_location[1], user_location[0], c='red', s=150, \n",
    "                  marker='X', label='Your Location', zorder=6)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Bike Stations Map')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_seasonal_patterns(df):\n",
    "    set_plot_style()\n",
    "    season_map = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n",
    "    df_temp = df.copy()\n",
    "    df_temp['season_name'] = df_temp['season'].map(season_map)\n",
    "    seasonal_avg = df_temp.groupby('season_name')['trip_count'].mean()\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['#90EE90', '#FFD700', '#FFA500', '#87CEEB']\n",
    "    bars = ax.bar(seasonal_avg.index, seasonal_avg.values, color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('Season')\n",
    "    ax.set_ylabel('Average Trip Count')\n",
    "    ax.set_title('Average Bike Trips by Season')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.2f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Load and Explore Data\n",
    "\n",
    "**Note:** Download the dataset from Kaggle and update the path below:\n",
    "https://www.kaggle.com/datasets/amykzhang/bergen-bike-sharing-dataset-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'bergen_merged.csv'\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Dataset Shape: {raw_df.shape}\")\n",
    "print(f\"Total Records: {raw_df.shape[0]:,}\")\n",
    "print(f\"Features: {raw_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "print(\"=\" * 50)\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 Records:\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary:\")\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values:\")\n",
    "missing = raw_df.isnull().sum()\n",
    "missing_pct = (missing / len(raw_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Count': missing, 'Percentage': missing_pct})\n",
    "missing_df[missing_df['Count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Stations: {raw_df['start_station_id'].nunique()}\")\n",
    "print(f\"Date Range: {raw_df['start_time'].min()} to {raw_df['start_time'].max()}\")\n",
    "print(f\"Average Trip Duration: {raw_df['duration'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting preprocessing pipeline...\")\n",
    "processed_df = preprocess_pipeline(DATA_PATH)\n",
    "\n",
    "print(f\"\\nOriginal Records: {len(raw_df):,}\")\n",
    "print(f\"After Preprocessing: {len(processed_df):,}\")\n",
    "print(f\"Records Removed: {len(raw_df) - len(processed_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Temporal Features:\")\n",
    "processed_df[['start_time', 'hour', 'day', 'month', 'dayofweek', 'is_rush_hour']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "hourly_trips = processed_df.groupby('hour').size()\n",
    "axes[0, 0].bar(hourly_trips.index, hourly_trips.values, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Total Trips by Hour of Day')\n",
    "axes[0, 0].set_xlabel('Hour')\n",
    "axes[0, 0].set_ylabel('Number of Trips')\n",
    "axes[0, 0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "daily_trips = processed_df.groupby('dayofweek').size()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0, 1].bar(range(7), daily_trips.values, color='forestgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Total Trips by Day of Week')\n",
    "axes[0, 1].set_xlabel('Day')\n",
    "axes[0, 1].set_ylabel('Number of Trips')\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(day_names)\n",
    "\n",
    "axes[1, 0].scatter(processed_df['temperature'], processed_df['duration'], alpha=0.1, s=1)\n",
    "axes[1, 0].set_title('Temperature vs Trip Duration')\n",
    "axes[1, 0].set_xlabel('Temperature (Â°C)')\n",
    "axes[1, 0].set_ylabel('Duration (seconds)')\n",
    "\n",
    "season_map = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n",
    "season_trips = processed_df['season'].map(season_map).value_counts()\n",
    "colors = ['#90EE90', '#FFD700', '#FFA500', '#87CEEB']\n",
    "axes[1, 1].pie(season_trips.values, labels=season_trips.index, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Trips by Season')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stations = processed_df['start_station_name'].value_counts().head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(range(len(top_stations)), top_stations.values, color='coral', edgecolor='black')\n",
    "ax.set_yticks(range(len(top_stations)))\n",
    "ax.set_yticklabels(top_stations.index)\n",
    "ax.set_xlabel('Number of Trips')\n",
    "ax.set_title('Top 10 Most Popular Stations')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2, f'{int(width):,}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting feature engineering pipeline...\")\n",
    "features_df = feature_engineering_pipeline(processed_df)\n",
    "\n",
    "print(f\"\\nFeature DataFrame Shape: {features_df.shape}\")\n",
    "print(f\"Total Samples (station-hour combinations): {len(features_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Columns:\")\n",
    "feature_cols = get_feature_columns()\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of Engineered Features:\")\n",
    "features_df[['start_station_name', 'date', 'hour', 'trip_count', \n",
    "             'trip_count_lag_1', 'trip_count_rolling_3h', 'temperature', \n",
    "             'is_rush_hour', 'station_avg_trips']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weather_correlation(features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_patterns(features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_patterns(features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = prepare_train_test_split(features_df, test_ratio=0.2)\n",
    "train_df, val_df = prepare_train_test_split(train_val_df, test_ratio=0.25)\n",
    "\n",
    "print(f\"Training Set: {len(train_df):,} samples\")\n",
    "print(f\"Validation Set: {len(val_df):,} samples\")\n",
    "print(f\"Test Set: {len(test_df):,} samples\")\n",
    "print(f\"\\nTotal: {len(train_df) + len(val_df) + len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['trip_count']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['trip_count']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['trip_count']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline Random Forest...\")\n",
    "rf_baseline = train_random_forest(X_train, y_train)\n",
    "\n",
    "rf_baseline_metrics = evaluate_model(rf_baseline, X_val, y_val)\n",
    "print(\"\\nBaseline Random Forest Performance (Validation):\")\n",
    "for metric, value in rf_baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline XGBoost...\")\n",
    "xgb_baseline = train_xgboost(X_train, y_train)\n",
    "\n",
    "xgb_baseline_metrics = evaluate_model(xgb_baseline, X_val, y_val)\n",
    "print(\"\\nBaseline XGBoost Performance (Validation):\")\n",
    "for metric, value in xgb_baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning Random Forest hyperparameters...\")\n",
    "rf_tuned, rf_best_params = hyperparameter_tuning_rf(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(f\"\\nBest Random Forest Parameters:\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "rf_tuned_metrics = evaluate_model(rf_tuned, X_val, y_val)\n",
    "print(\"\\nTuned Random Forest Performance (Validation):\")\n",
    "for metric, value in rf_tuned_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost hyperparameters...\")\n",
    "xgb_tuned, xgb_best_params = hyperparameter_tuning_xgb(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(f\"\\nBest XGBoost Parameters:\")\n",
    "for param, value in xgb_best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "xgb_tuned_metrics = evaluate_model(xgb_tuned, X_val, y_val)\n",
    "print(\"\\nTuned XGBoost Performance (Validation):\")\n",
    "for metric, value in xgb_tuned_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Model Comparison & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RF_Baseline': rf_baseline,\n",
    "    'RF_Tuned': rf_tuned,\n",
    "    'XGB_Baseline': xgb_baseline,\n",
    "    'XGB_Tuned': xgb_tuned\n",
    "}\n",
    "\n",
    "comparison_results = compare_models(models, X_test, y_test)\n",
    "print(\"Model Comparison on Test Set:\")\n",
    "print(\"=\" * 50)\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(comparison_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = comparison_results['RMSE'].idxmin()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test RMSE: {comparison_results.loc[best_model_name, 'RMSE']:.4f}\")\n",
    "print(f\"Test MAE: {comparison_results.loc[best_model_name, 'MAE']:.4f}\")\n",
    "print(f\"Test RÂ²: {comparison_results.loc[best_model_name, 'R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = get_feature_importance(best_model, feature_cols)\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "importance_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(importance_df, top_n=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Prediction Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "plot_actual_vs_predicted(y_test.values, y_pred_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_test.values, y_pred_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.abs(y_test.values - y_pred_test)\n",
    "error_percentiles = np.percentile(errors, [25, 50, 75, 90, 95, 99])\n",
    "\n",
    "print(\"Error Distribution:\")\n",
    "print(f\"  25th percentile: {error_percentiles[0]:.2f} trips\")\n",
    "print(f\"  50th percentile (median): {error_percentiles[1]:.2f} trips\")\n",
    "print(f\"  75th percentile: {error_percentiles[2]:.2f} trips\")\n",
    "print(f\"  90th percentile: {error_percentiles[3]:.2f} trips\")\n",
    "print(f\"  95th percentile: {error_percentiles[4]:.2f} trips\")\n",
    "print(f\"  99th percentile: {error_percentiles[5]:.2f} trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 17. Station Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_latitude = 60.3913\n",
    "user_longitude = 5.3221\n",
    "target_hour = 8\n",
    "\n",
    "print(f\"User Location: ({user_latitude}, {user_longitude})\")\n",
    "print(f\"Target Hour: {target_hour}:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_best_station(\n",
    "    user_lat=user_latitude,\n",
    "    user_lon=user_longitude,\n",
    "    target_hour=target_hour,\n",
    "    stations_info=features_df,\n",
    "    features_df=features_df,\n",
    "    model=best_model,\n",
    "    feature_cols=feature_cols,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(format_recommendation_output(recommendations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations = features_df[['start_station_id', 'start_station_name', \n",
    "                                'start_station_latitude', 'start_station_longitude']].drop_duplicates()\n",
    "\n",
    "plot_station_map(unique_stations, recommendations, \n",
    "                 user_location=(user_latitude, user_longitude))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 18. Model Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_analysis = test_df.copy()\n",
    "test_df_analysis['prediction'] = y_pred_test\n",
    "test_df_analysis['absolute_error'] = np.abs(test_df_analysis['trip_count'] - test_df_analysis['prediction'])\n",
    "\n",
    "high_error_threshold = np.percentile(test_df_analysis['absolute_error'], 95)\n",
    "high_error_cases = test_df_analysis[test_df_analysis['absolute_error'] > high_error_threshold]\n",
    "\n",
    "print(f\"High Error Cases (top 5% errors, threshold: {high_error_threshold:.2f}):\")\n",
    "print(f\"Number of cases: {len(high_error_cases)}\")\n",
    "print(\"\\nSample of high-error predictions:\")\n",
    "high_error_cases[['start_station_name', 'date', 'hour', 'trip_count', 'prediction', \n",
    "                  'absolute_error', 'temperature', 'precipitation', 'is_rush_hour']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis of High Error Cases:\")\n",
    "print(f\"  Average actual trips: {high_error_cases['trip_count'].mean():.2f}\")\n",
    "print(f\"  Average predicted trips: {high_error_cases['prediction'].mean():.2f}\")\n",
    "print(f\"  Rush hour percentage: {high_error_cases['is_rush_hour'].mean()*100:.1f}%\")\n",
    "print(f\"  Rainy conditions percentage: {(high_error_cases['precipitation'] > 0).mean()*100:.1f}%\")\n",
    "print(f\"  Holiday percentage: {high_error_cases['is_holiday'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 19. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rush_hour_trips = features_df[features_df['is_rush_hour'] == 1]['trip_count'].mean()\n",
    "non_rush_trips = features_df[features_df['is_rush_hour'] == 0]['trip_count'].mean()\n",
    "\n",
    "print(\"HYPOTHESIS 1: Higher activity during rush hours\")\n",
    "print(f\"  Average trips during rush hours: {rush_hour_trips:.2f}\")\n",
    "print(f\"  Average trips during non-rush hours: {non_rush_trips:.2f}\")\n",
    "print(f\"  Difference: {rush_hour_trips - non_rush_trips:.2f} more trips during rush hours\")\n",
    "print(f\"  Result: {'CONFIRMED' if rush_hour_trips > non_rush_trips else 'NOT CONFIRMED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_correlation = features_df[['temperature', 'trip_count']].corr().iloc[0, 1]\n",
    "\n",
    "print(\"\\nHYPOTHESIS 2: Temperature affects bike rentals\")\n",
    "print(f\"  Correlation between temperature and trips: {temp_correlation:.4f}\")\n",
    "print(f\"  Result: {'CONFIRMED' if abs(temp_correlation) > 0.1 else 'NOT CONFIRMED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainy_trips = features_df[features_df['precipitation'] > 0]['trip_count'].mean()\n",
    "dry_trips = features_df[features_df['precipitation'] == 0]['trip_count'].mean()\n",
    "\n",
    "print(\"\\nHYPOTHESIS 3: Rain reduces bike rentals\")\n",
    "print(f\"  Average trips during rain: {rainy_trips:.2f}\")\n",
    "print(f\"  Average trips when dry: {dry_trips:.2f}\")\n",
    "print(f\"  Reduction: {((dry_trips - rainy_trips) / dry_trips * 100):.1f}%\")\n",
    "print(f\"  Result: {'CONFIRMED' if rainy_trips < dry_trips else 'NOT CONFIRMED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_trips = features_df[features_df['is_weekend'] == 0]['trip_count'].mean()\n",
    "weekend_trips = features_df[features_df['is_weekend'] == 1]['trip_count'].mean()\n",
    "\n",
    "print(\"\\nHYPOTHESIS 4: Weekend patterns differ from weekdays\")\n",
    "print(f\"  Average weekday trips: {weekday_trips:.2f}\")\n",
    "print(f\"  Average weekend trips: {weekend_trips:.2f}\")\n",
    "print(f\"  Difference: {abs(weekday_trips - weekend_trips):.2f} trips\")\n",
    "print(f\"  Result: {'CONFIRMED' if abs(weekday_trips - weekend_trips) > 0.5 else 'NOT CONFIRMED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 20. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model, f'{best_model_name}.joblib')\n",
    "print(f\"Best model saved as {best_model_name}.joblib\")\n",
    "\n",
    "comparison_results.to_csv('model_comparison_results.csv')\n",
    "print(\"Model comparison results saved as model_comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 21. Conclusions & Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: Our best model achieves strong predictive performance with temporal and weather features\n",
    "2. **Important Features**: Lag features and rolling averages are crucial predictors\n",
    "3. **Weather Impact**: Temperature and precipitation significantly affect bike usage\n",
    "4. **Temporal Patterns**: Clear daily and seasonal patterns exist in bike sharing\n",
    "\n",
    "### Recommendations for Operators:\n",
    "\n",
    "1. **Pre-position bikes** during predicted low-availability periods\n",
    "2. **Weather-based planning** for bike redistribution\n",
    "3. **Rush hour management** with proactive rebalancing\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. Include real-time station capacity data\n",
    "2. Add event-based features (concerts, sports events)\n",
    "3. Integrate traffic and public transport data\n",
    "4. Develop a mobile app for real-time recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Diagram\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Raw Data  â”‚ -> â”‚  Preprocess  â”‚ -> â”‚   Feature     â”‚\n",
    "â”‚   Loading   â”‚    â”‚  (Clean,     â”‚    â”‚  Engineering  â”‚\n",
    "â”‚             â”‚    â”‚   Parse)     â”‚    â”‚  (Lags, Rolls)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                              â”‚\n",
    "                                              v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Model     â”‚ <- â”‚    Model     â”‚ <- â”‚  Train/Val/   â”‚\n",
    "â”‚   Saving    â”‚    â”‚   Training   â”‚    â”‚  Test Split   â”‚\n",
    "â”‚             â”‚    â”‚  (RF, XGB)   â”‚    â”‚               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                          v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Station    â”‚ <- â”‚   Predict    â”‚ <- â”‚   Evaluate    â”‚\n",
    "â”‚  Recommend  â”‚    â”‚   Trips/Hour â”‚    â”‚  (RMSE, MAE)  â”‚\n",
    "â”‚             â”‚    â”‚              â”‚    â”‚               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total features engineered: {len(feature_cols)}\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Final Test RMSE: {comparison_results.loc[best_model_name, 'RMSE']:.4f}\")\n",
    "print(f\"Final Test MAE: {comparison_results.loc[best_model_name, 'MAE']:.4f}\")\n",
    "print(f\"Final Test RÂ²: {comparison_results.loc[best_model_name, 'R2']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
